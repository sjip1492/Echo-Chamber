{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjip1492/Echo-Chamber/blob/master/Weekly_Activity_Sound_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Ttw4__FqqV"
      },
      "source": [
        "*Credit: This notebook was prepared by\n",
        " Payam Jome Yazdian (Rosie Lab)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y76axBV8GQo2"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhxzzOkz0fyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd477a34-9801-48e2-965c-c59aa378ad4b"
      },
      "source": [
        "!pip install soundfile\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavef\n",
        "import math\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio\n",
        "\n",
        "from google.colab import drive\n",
        "import soundfile as sf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTuCSBCUGbx9"
      },
      "source": [
        "# Task 1. Generate a sound:\n",
        "A single-frequency sound wave with frequency f, maximum amplitude A, and phase θ is represented by the sine function\n",
        "\n",
        "y=Asin(2πfx+θ)\n",
        "\n",
        "where x is time and y is the amplitude of the sound wave at time x.\n",
        "\n",
        "Given the formalization above, complete the following function *mkwave* so as to generate a sine wave signal with a specific duration, amplitude, frequency (Hz), phase, and sampling rate.\n",
        "\n",
        "Play around with the parameters and answer the following questions in Canvas:    \n",
        "\n",
        "1.   What is the relationship between frequency (Hz) and the perception of the resulting sound?\n",
        "2.   What is the relationship between the length of a sound and sampling rate?\n",
        "3.   How does sampling rate affect the sound?\n",
        "4.   How does phase affect the sound?\n",
        "\n",
        "For more information about sampling rate please check [this](https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKPw96VI09q0",
        "outputId": "c553ecab-2180-465a-e860-8e225f75f67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "def mkwave(duration_sec, amp, freq_hz, phase, sampling_rate=44100):\n",
        "    x = np.arange(0.0, duration_sec, 1.0/sampling_rate) # start, stop, step\n",
        "    y = # Your code here\n",
        "    # ...\n",
        "    return y\n",
        "\n",
        "target_freq_hz = # ex. 440.0\n",
        "sampling_rate = # ex. 44100\n",
        "duration = 1.0\n",
        "amplitude = 1.0\n",
        "phase = 0\n",
        "\n",
        "wav = mkwave(duration, amplitude, target_freq_hz, phase, sampling_rate)\n",
        "\n",
        "#Plot signal\n",
        "plt.figure(1)\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.title(\"Wave Signal\")\n",
        "plt.plot(wav)\n",
        "plt.show()\n",
        "\n",
        "#Play audio\n",
        "ipd.display(ipd.Audio(data=wav, rate=sampling_rate))\n",
        "\n",
        "# Write to a file\n",
        "wavef.write(\"example.wav\", sampling_rate, wav)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-5525531de12a>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-5525531de12a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    y = # Your code here\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SigXvLyXGkHT"
      },
      "source": [
        "## Task 2. Visualize a wave signal\n",
        "- In this part you must implement a part of code which visualizes the following plots of generated signal by previous section.\n",
        "-- spectrum\n",
        "\n",
        "Hint: You may use the librosa's [feature](https://librosa.org/doc/main/feature.html) class.\n",
        "\n",
        "Post a picture of your spectrogram in Canvas, and answer the following question.\n",
        "\n",
        "What does the spectrogram allow us to see, with respect to the parameters you set in Step 1?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OATlUbaMFpdH"
      },
      "source": [
        "# plot spectogram:\n",
        "# ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMRd8EZZgAQu"
      },
      "source": [
        "# Task 3. Plot the spectrum of 2 wave files of your choice\n",
        "\n",
        "Find or record at least 2 short [affect bursts](https://www.sciencedirect.com/science/article/pii/S016763930200078X?casa_token=bdo4xIj0JXYAAAAA:zJkZiqBgxB9MJbNtZyBPBD5kVZzfpS8MUXJGAlLmhyPHrXtu0_uimWsfEZKFc9rnJhHX83FtZw), such as \"oooOOOOoo\" or \"ah!\" or \"awwww\", representing an emotion(s). For example, what sound could you use to express disgust or awe?\n",
        "\n",
        "### Load an audio file from Google Drive:\n",
        "This allows you to work in CoLab without needing to upload your data at each runtime\n",
        "-  First, open the file browser on the left hand side. It will show a **'Mount Drive'** button similar to the Google Drive Icon.\n",
        "- Once clicked, you'll see a permissions prompt to mount Drive, and then your Drive files will be present.\n",
        "- Afterward, look for your intended file, right-click on that, and click \"Copy path\".\n",
        "- Finally, paste it in your code (inspired from the above) instead of *myfile.wav*\n",
        "\n",
        "### OR, load an audio file locally\n",
        "- If using Jupyter Notebook offline, simply paste the path to the file\n",
        "\n",
        "Note: You can download a sample from https://freesound.org/ or record your own.\n",
        "\n",
        "Post the picture of your spectrums in Canvas and answer the following questions.\n",
        "\n",
        "1. What sounds did you choose?\n",
        "2. Describe what you see in the spectrum(s). What formants do you see?\n",
        "3. Any other comments on this activity."
      ]
    }
  ]
}